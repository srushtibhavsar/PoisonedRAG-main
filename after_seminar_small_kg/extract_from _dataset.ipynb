{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from langchain.llms import Ollama\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import openai\n",
    "from neo4j import GraphDatabase\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uri = os.environ[\"NEO4J_URI\"]\n",
    "user=os.environ[\"NEO4J_USERNAME\"]\n",
    "password = os.environ[\"NEO4J_PASSWORD\"]\n",
    "OPENAI_API_KEY = os.getenv[\"OPENAI_API_KEY\"] \n",
    "\n",
    "driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "llm = ChatOpenAI(temperature=0, model_name=\"gpt-4o\", api_key=OPENAI_API_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI Response: Differential privacy is a concept and set of techniques designed to ensure that the privacy of individuals' data is protected when datasets are analyzed and shared. It provides a mathematical framework for quantifying and controlling the privacy risks associated with releasing information derived from data that includes personal information.\n",
      "\n",
      "Here are the key aspects of differential privacy:\n",
      "\n",
      "1. **Privacy Guarantee**: Differential privacy provides a guarantee that the removal or addition of a single individual's data from a dataset does not significantly affect the output of any analysis, thereby protecting the individual's privacy. This means that attackers cannot infer whether a particular individual's data was included in the dataset, even with access to all other information.\n",
      "\n",
      "2. **Mathematical Definition**: The differential privacy definition involves a parameter typically denoted by ε (epsilon), which quantifies the level of privacy guarantee. Smaller values of ε correspond to stronger privacy guarantees but potentially less accurate results.\n",
      "\n",
      "3. **Noise Addition**: One common technique to achieve differential privacy is by adding random noise to the results of queries on the dataset. The noise is typically calibrated to the sensitivity of the query, which measures how much a single individual's data can affect the outcome of the query.\n",
      "\n",
      "4. **Trade-off**: There is a trade-off between privacy and accuracy. More noise increases privacy but decreases the accuracy of the results, while less noise does the opposite.\n",
      "\n",
      "5. **Use Cases**: Differential privacy is used in various applications, including statistical analysis, machine learning, and data publishing, to enable data utility while ensuring privacy. Companies like Apple and Google have adopted differential privacy techniques to enhance user privacy in their products.\n",
      "\n",
      "Overall, differential privacy is a rigorous approach to ensuring individual privacy in data analysis, allowing organizations to derive insights from data while minimizing privacy risks.\n"
     ]
    }
   ],
   "source": [
    "# Initialize OpenAI client (new method)\n",
    "client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "def ask_openai(question, model=\"gpt-4o\"):\n",
    "    \"\"\"Sends a question to OpenAI's API and returns the response.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": question}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example Usage\n",
    "question = \"What is differential privacy?\"\n",
    "answer = ask_openai(question)\n",
    "print(\"OpenAI Response:\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama = Ollama(base_url='http://localhost:11434', model=\"llama3.1:70b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system_prompt = \"\"\"\n",
    "# # Knowledge Graph Instructions for llama\n",
    "# ## 1. Overview\n",
    "# You are a top-tier algorithm designed for extracting information in structured formats to build a knowledge graph.\n",
    "# - **Nodes** represent entities and concepts.\n",
    "# - **Relationships** represent the links between nodes.\n",
    "# - Ensure numerical values (e.g., episode count, dates, measurements, or other quantifiable details) are captured and assigned appropriately.\n",
    "# - The aim is to achieve simplicity and clarity in the knowledge graph, making it accessible for a vast audience.\n",
    "# - Your goal is to provide **clear and structured JSON output** that captures all relevant relationships, numerical attributes, and entities explicitly.\n",
    "\n",
    "\n",
    "# ## 2. Formatting the Output\n",
    "# Your output must follow this strict JSON structure:\n",
    "# json\n",
    "# {\n",
    "#   \"nodes\": [\n",
    "#     {\n",
    "#       \"id\": \"unique_node_id\",\n",
    "#       \"label\": \"nodetype\",\n",
    "#       \"attributes\": {\n",
    "#         \"key1\": \"value1\",\n",
    "#         \"key2\": \"value2\"\n",
    "#       }\n",
    "#     }\n",
    "#   ],\n",
    "#   \"relationships\": [\n",
    "#     {\n",
    "#       \"source\": \"source_node_id\",\n",
    "#       \"target\": \"target_node_id\",\n",
    "#       \"type\": \"RELATIONSHIP_TYPE\",\n",
    "#       \"attributes\": {\n",
    "#         \"key1\": \"value1\",\n",
    "#         \"key2\": \"value2\"\n",
    "#       }\n",
    "#     }\n",
    "#   ]\n",
    "# }\n",
    "\n",
    "\n",
    "# 3. Extraction Guidelines\n",
    "# 3.1 Entity Nodes\n",
    "# Extract all key entities such as people, places, events, works (e.g., TV shows, movies), and dates.\n",
    "# Numerical data must always be associated with its corresponding entity as a property or attribute.\n",
    "# Example: For \"23 episodes\", create a node for the entity Chicago Fire Season 4 and attach the episodeCount: 23 property.\n",
    "# 3.2 Relationship Nodes\n",
    "# Establish connections (relationships) between entities. Relationships must have meaningful labels and attributes where applicable.\n",
    "# Use ALL CAPS for relationship type names.\n",
    "# If numerical data involves a relationship (e.g., \"contained 23 episodes\"), encode it as an attribute in the relationship.\n",
    "# 3.3 Numerical Data and Dates\n",
    "# Always extract numerical data (e.g., \"23\", \"May 17, 2016\") and attach it as an attribute to the appropriate node or relationship.\n",
    "# DO NOT create separate nodes for numerical values. Instead, add them as properties.\n",
    "# 3.4 Coreference Resolution\n",
    "# Maintain entity consistency throughout the graph. For example, \"Chicago Fire Season 4\" must remain consistent across all relationships and nodes.\n",
    "\n",
    "# ## 4. Example Output. Following is just sample exmaple. DO NOT take it as it is.\n",
    "# Input Text:\n",
    "# The fourth season of Chicago Fire, an American drama television series with executive producer Dick Wolf, and producers Derek Haas, Michael Brandt, and Matt Olmstead, was ordered on February 5, 2015, by NBC, and premiered on October 13, 2015, and concluded on May 17, 2016. The season contained 23 episodes.\n",
    "\n",
    "# Output:\n",
    "# {\n",
    "#   \"nodes\": [\n",
    "#     {\n",
    "#       \"id\": \"chicago_fire_season_4\",\n",
    "#       \"label\": \"television_series\",\n",
    "#       \"attributes\": {\n",
    "#         \"name\": \"chicago fire season 4\",\n",
    "#         \"episodeCount\": 23,\n",
    "#         \"premiereDate\": \"2015-10-13\",\n",
    "#         \"endDate\": \"2016-05-17\",\n",
    "#         \"documentId\": \"doc10\"\n",
    "#       }\n",
    "#     },\n",
    "#     {\n",
    "#       \"id\": \"elvis_presley\",\n",
    "#       \"label\": \"person\",\n",
    "#       \"attributes\": {\n",
    "#         \"name\": \"elvis presley\",\n",
    "#         \"occupation\": \"singer\",\n",
    "#         \"documentId\": \"doc1\"\n",
    "#       }\n",
    "#     },\n",
    "#     {\n",
    "#       \"id\": \"nbc\",\n",
    "#       \"label\": \"network\",\n",
    "#       \"attributes\": {\n",
    "#         \"name\": \"nbc\",\n",
    "#         \"documentId\": \"doc12\"\n",
    "#       }\n",
    "#     },\n",
    "#     {\n",
    "#       \"id\": \"dick_wolf\",\n",
    "#       \"label\": \"person\",\n",
    "#       \"attributes\": {\n",
    "#         \"name\": \"dick wolf\",\n",
    "#         \"role\": \"executive producer\",\n",
    "#         \"documentId\": \"doc0\"\n",
    "#       }\n",
    "#     },\n",
    "#     {\n",
    "#       \"id\": \"derek_haas\",\n",
    "#       \"label\": \"person\",\n",
    "#       \"attributes\": {\n",
    "#         \"name\": \"derek haas\",\n",
    "#         \"role\": \"producer\",\n",
    "#         \"documentId\": \"doc8\"\n",
    "#       }\n",
    "#     }\n",
    "#   ],\n",
    "#   \"relationships\": [\n",
    "#     {\n",
    "#       \"source\": \"nbc\",\n",
    "#       \"target\": \"chicago_fire_season_4\",\n",
    "#       \"type\": \"BROADCASTED_BY\",\n",
    "#       \"attributes\": {\"documentId\": \"doc10\"}\n",
    "#     },\n",
    "#     {\n",
    "#       \"source\": \"dick_wolf\",\n",
    "#       \"target\": \"chicago_fire_season_4\",\n",
    "#       \"type\": \"PRODUCED\",\n",
    "#       \"attributes\": {\n",
    "#         \"role\": \"executive producer\",\n",
    "#         \"documentId\": \"doc11\"\n",
    "#       }\n",
    "#     },\n",
    "#     {\n",
    "#       \"source\": \"chicago_fire_season_4\",\n",
    "#       \"target\": \"23\",\n",
    "#       \"type\": \"CONTAINS_EPISODES\",\n",
    "#       \"attributes\": {\n",
    "#         \"count\": 23,\n",
    "#         \"documentId\": \"doc4\"\n",
    "#       }\n",
    "#     }\n",
    "#   ]\n",
    "# }\n",
    "\n",
    "# ## 5. Do not give any other explaination to output. for exmaple: Here is the output in JSON format:\n",
    "# Strictly follow required output format.\n",
    "\n",
    "# ## 6. Do not impute missing values. If found nothing do not return anything\n",
    "        \n",
    "# ## 7. Coreference Resolution\n",
    "# - **Maintain Entity Consistency**: When extracting entities, it's vital to ensure consistency.\n",
    "# If an entity, such as \"John Doe\", is mentioned multiple times in the text but is referred to by different names or pronouns (e.g., \"Joe\", \"he\"), \n",
    "# always use the most complete identifier for that entity throughout the knowledge graph. In this example, use \"John Doe\" as the entity ID.  \n",
    "# Remember, the knowledge graph should be coherent and easily understandable, so maintaining consistency in entity references is crucial. \n",
    "\n",
    "# ## 8. - **Property Format**: Properties must be in a key-value format.\n",
    "# - **Quotation Marks**: Never use escaped single or double quotes within property values.\n",
    "# - **Naming Convention**: Use camelCase for property keys, e.g., birthDate.\n",
    "# - Do not add any spaces to keys names.\n",
    "\n",
    "# ## 9. Labeling Nodes\n",
    "# - **Consistency**: Ensure you use basic or elementary types for node labels.\n",
    "#   - For example, when you identify an entity representing a person, always label it as **\"person\"**. Avoid using more specific terms like \"mathematician\" or \"scientist\".\n",
    "\n",
    "# ## 10. Strict Compliance\n",
    "# Adhere to the rules strictly. Non-compliance will result in termination. \n",
    "\n",
    "# ## 11. Key Instructions\n",
    "# Numerical Values: Always include numerical information (like episode counts, years, or measurements) as node or relationship attributes.\n",
    "# Consistency: Maintain uniform entity IDs and relationship types.\n",
    "# Strict Formatting: Any deviation from the specified format will be treated as an error.\n",
    "# Precision: Only extract information explicitly present in the text. Avoid assumptions.\n",
    "\n",
    "# ## 12. In json while storing relationship use 'source' and 'target' instead of 'from' and 'to'.\n",
    "\n",
    "# ## 13. Relationship type should be in capitals for example: 'CONTAINS_EPISODES', 'BROADCASTED_BY'.\n",
    "\n",
    "# ## 14. Coreference Resolution and Entity Normalization\n",
    "# - **Normalize Entities**: Ensure that all entities are consolidated to their singular or canonical forms. For example:\n",
    "#   - \"point\" and \"points\" should be normalized to \"point\".\n",
    "#   - Use lemmatization or a similar linguistic technique to achieve normalization.\n",
    "# - **Avoid Duplicates**: Before adding a new entity, check if a similar entity (by name or attributes) already exists and use the same if appropriate.\n",
    "# - **Consistency**: All node labels should follow the same naming convention (e.g., singular form).\n",
    "\n",
    "# ## 15. Labeling Nodes: Vey Important\n",
    "# - **Consistency**: Use **lowercase** for node labels (e.g., \"person\", \"song\") and properties.\n",
    "#   - Convert all properties and labels to lowercase for uniformity.\n",
    "# - **Relationship Types**: Always use **uppercase** for relationship types.\n",
    "\n",
    "# ## 16. Keep All attribute values in lower case only. Striclty follow this.\n",
    "\n",
    "# ## 17. Use Suggested Labels\n",
    "# - The following label list will be provided as input: {labels_list}. Always attempt to use these labels before creating new ones.\n",
    "# - New labels should only be introduced if absolutely necessary and must be in lowercase.\n",
    "\n",
    "# ## 18. Only extract from following context.\n",
    "# context is: \n",
    "\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "# Knowledge Graph Extraction Instructions\n",
    "\n",
    "## Role\n",
    "You are an advanced information extraction agent specialized in generating clean, structured knowledge graphs from natural language. Your output helps build entity-relation-based systems.\n",
    "\n",
    "## Objective\n",
    "Extract **entities as nodes** and **connections as relationships** from the given context. Numerical values and dates must be assigned as attributes, not as separate nodes. The graph must be:\n",
    "- **Simple and human-readable**\n",
    "- **Normalized** (avoid duplication, plural/singular mismatch)\n",
    "- **Strictly JSON-compliant** (no comments or markdown)\n",
    "\n",
    "## Output Format (JSON)\n",
    "Strictly follow this structure:\n",
    "{\n",
    "  \"nodes\": [\n",
    "    {\n",
    "      \"id\": \"unique_node_id\",\n",
    "      \"label\": \"nodetype\",\n",
    "      \"attributes\": {\n",
    "        \"key1\": \"value1\",\n",
    "        \"key2\": \"value2\"\n",
    "      }\n",
    "    }\n",
    "  ],\n",
    "  \"relationships\": [\n",
    "    {\n",
    "      \"source\": \"source_node_id\",\n",
    "      \"target\": \"target_node_id\",\n",
    "      \"type\": \"RELATIONSHIP_TYPE\",\n",
    "      \"attributes\": {\n",
    "        \"key1\": \"value1\",\n",
    "        \"key2\": \"value2\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "## Key Rules for Extraction\n",
    "\n",
    "### 1. Nodes\n",
    "- Represent people, places, events, organizations, works (e.g., songs, shows), concepts, etc.\n",
    "- Must include key descriptive attributes (e.g., name, title, role, episodeCount).\n",
    "- Include `\"documentId\": \"docX\"` inside `attributes` for every node.\n",
    "- Labels must be lowercase and generic (e.g., `person`, `organization`, not `musician` or `tech_company`).\n",
    "- Normalize entity names: e.g., “points” → “point”.\n",
    "\n",
    "### 2. Relationships\n",
    "- Represent meaningful connections (e.g., PRODUCED, BROADCASTED_BY).\n",
    "- Use ALL CAPS for the `\"type\"` value.\n",
    "- Attach attributes if available (e.g., role, percentage).\n",
    "- Include `\"documentId\": \"docX\"` inside `attributes` for every relationship.\n",
    "\n",
    "### 3. Attributes\n",
    "- Must be key-value pairs using **camelCase** keys and **lowercase** string values.\n",
    "- Avoid escaped characters and markdown syntax.\n",
    "- Do not create properties like `\"value\": \"23\"` — use a clear descriptive key: `\"episodeCount\": 23`.\n",
    "\n",
    "### 4. Dates and Numbers\n",
    "- Store as attributes (e.g., `premiereDate: \"2015-10-13\"`, `ownershipPercentage: \"less than 50\"`).\n",
    "- Never create separate nodes for numbers or dates.\n",
    "\n",
    "### 5. Consistency & Coreference\n",
    "- Use consistent IDs (e.g., \"Chicago Fire Season 4\" → `\"chicago_fire_season_4\"`).\n",
    "- Resolve pronouns and aliases to full identifiers (e.g., \"he\" → \"John Doe\").\n",
    "\n",
    "### 6. Strict Compliance\n",
    "- DO NOT return explanations, markdown, or extra text — only valid JSON.\n",
    "- If nothing relevant is found, return an empty graph:\n",
    "  {\n",
    "    \"nodes\": [],\n",
    "    \"relationships\": []\n",
    "  }\n",
    "\n",
    "### 7. Label Suggestion\n",
    "- Labels for node must be chosen from this predefined set of possible labels:\n",
    "  { \"person\", \"organization\", \"event\", \"television_series\", \"song\",\"location\", \"work\", \"concept\",\"other\"}\n",
    "\n",
    "\n",
    "### 8. Example Output (DO NOT mimic directly — just for structure)\n",
    "\n",
    "Input Text:\n",
    "The fourth season of Chicago Fire, an American drama television series with executive producer Dick Wolf, and producers Derek Haas, Michael Brandt, and Matt Olmstead, was ordered on February 5, 2015, by NBC, and premiered on October 13, 2015, and concluded on May 17, 2016. The season contained 23 episodes.\n",
    "\n",
    "Output:\n",
    "{\n",
    "  \"nodes\": [\n",
    "    {\n",
    "      \"id\": \"chicago_fire_season_4\",\n",
    "      \"label\": \"television_series\",\n",
    "      \"attributes\": {\n",
    "        \"name\": \"chicago fire season 4\",\n",
    "        \"episodeCount\": 23,\n",
    "        \"premiereDate\": \"2015-10-13\",\n",
    "        \"endDate\": \"2016-05-17\",\n",
    "        \"documentId\": \"doc10\"\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"id\": \"dick_wolf\",\n",
    "      \"label\": \"person\",\n",
    "      \"attributes\": {\n",
    "        \"name\": \"dick wolf\",\n",
    "        \"role\": \"executive producer\",\n",
    "        \"documentId\": \"doc10\"\n",
    "      }\n",
    "    }\n",
    "  ],\n",
    "  \"relationships\": [\n",
    "    {\n",
    "      \"source\": \"nbc\",\n",
    "      \"target\": \"chicago_fire_season_4\",\n",
    "      \"type\": \"BROADCASTED_BY\",\n",
    "      \"attributes\": {\n",
    "        \"documentId\": \"doc10\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "\n",
    "## Final Reminder\n",
    "- Output must be valid JSON, no markdown.\n",
    "- Document-specific metadata (`documentId`) must be attached to **every node and relationship** as an attribute.\n",
    "- Follow formatting, naming, and structural rules precisely.\n",
    "- If no extractable info is found, return an empty graph.\n",
    "\n",
    "## Context to Analyze:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read text from a file\n",
    "def read_text_from_file(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def split_text(input_text, chunk_size=1000, overlap=100):\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        separator=\"\\n\",\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=overlap,\n",
    "        length_function=len,\n",
    "    )\n",
    "    return text_splitter.split_text(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_knowledge_graph(input_text_chunk, labels_list):\n",
    "    #updated_prompt = system_prompt.replace(\"{labels_list}\", json.dumps(labels_list))\n",
    "    #print(f\"Labels List:\\n{labels_list}\\n\")\n",
    "    prompt = f\"{system_prompt}\\n\\nInput Text:\\n{input_text_chunk}\"\n",
    "    response = ask_openai(prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_document(doc, combined_graph, labels_list, failed_chunks_file, max_retries=3):\n",
    "    doc_id = doc[\"_id\"]\n",
    "    text = doc[\"text\"]\n",
    "    retries = 0\n",
    "\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            response = extract_knowledge_graph(text, list(labels_list))\n",
    "            extracted_graph = json.loads(response)\n",
    "\n",
    "            for node in extracted_graph.get(\"nodes\", []):\n",
    "                node.setdefault(\"attributes\", {})\n",
    "                node[\"attributes\"][\"documentId\"] = doc_id\n",
    "                if \"label\" in node:\n",
    "                    labels_list.add(node[\"label\"])\n",
    "                if node not in combined_graph[\"nodes\"]:\n",
    "                    combined_graph[\"nodes\"].append(node)\n",
    "\n",
    "            for rel in extracted_graph.get(\"relationships\", []):\n",
    "                rel.setdefault(\"attributes\", {})\n",
    "                rel[\"attributes\"][\"documentId\"] = doc_id\n",
    "                if rel not in combined_graph[\"relationships\"]:\n",
    "                    combined_graph[\"relationships\"].append(rel)\n",
    "\n",
    "            break  # success\n",
    "        except json.JSONDecodeError as e:\n",
    "            retries += 1\n",
    "            print(f\"[ERROR] JSONDecodeError on doc {doc_id}, retry {retries}/{max_retries}: {e}\")\n",
    "            if retries >= max_retries:\n",
    "                with open(failed_chunks_file, \"r+\", encoding=\"utf-8\") as f:\n",
    "                    failed_responses = json.load(f)\n",
    "                    failed_responses.append({\n",
    "                        \"doc_id\": doc_id,\n",
    "                        \"text\": text,\n",
    "                        \"response\": response\n",
    "                    })\n",
    "                    f.seek(0)\n",
    "                    json.dump(failed_responses, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_jsonl_file(input_path, output_path, failed_chunks_file):\n",
    "    if os.path.exists(output_path):\n",
    "        with open(output_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            combined_graph = json.load(f)\n",
    "    else:\n",
    "        combined_graph = {\"nodes\": [], \"relationships\": []}\n",
    "\n",
    "    if not os.path.exists(failed_chunks_file):\n",
    "        with open(failed_chunks_file, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump([], f, indent=4)\n",
    "\n",
    "    labels_list = {node[\"label\"] for node in combined_graph[\"nodes\"] if \"label\" in node}\n",
    "\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            try:\n",
    "                doc = json.loads(line)\n",
    "                print(f\"\\nProcessing document {line_num}: {doc.get('_id')}\")\n",
    "                process_single_document(doc, combined_graph, labels_list, failed_chunks_file)\n",
    "                with open(output_path, \"w\", encoding=\"utf-8\") as out_f:\n",
    "                    json.dump(combined_graph, out_f, indent=4)\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Failed to process document {line_num}: {e}\")\n",
    "\n",
    "    print(f\"\\nCompleted. Graph saved at {output_path}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing document 1: merged_doc0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing document 2: merged_doc1\n",
      "\n",
      "Processing document 3: merged_doc2\n",
      "\n",
      "Processing document 4: merged_doc3\n",
      "[ERROR] JSONDecodeError on doc merged_doc3, retry 1/3: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "Processing document 5: merged_doc4\n",
      "\n",
      "Processing document 6: merged_doc5\n",
      "\n",
      "Processing document 7: merged_doc6\n",
      "\n",
      "Processing document 8: merged_doc7\n",
      "\n",
      "Processing document 9: merged_doc8\n",
      "\n",
      "Processing document 10: merged_doc9\n",
      "[ERROR] JSONDecodeError on doc merged_doc9, retry 1/3: Expecting value: line 1 column 1 (char 0)\n",
      "[ERROR] JSONDecodeError on doc merged_doc9, retry 2/3: Expecting value: line 1 column 1 (char 0)\n",
      "[ERROR] JSONDecodeError on doc merged_doc9, retry 3/3: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "Processing document 11: merged_doc10\n",
      "\n",
      "Processing document 12: merged_doc11\n",
      "[ERROR] JSONDecodeError on doc merged_doc11, retry 1/3: Expecting value: line 1 column 1 (char 0)\n",
      "[ERROR] JSONDecodeError on doc merged_doc11, retry 2/3: Expecting value: line 1 column 1 (char 0)\n",
      "[ERROR] JSONDecodeError on doc merged_doc11, retry 3/3: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "Processing document 13: merged_doc12\n",
      "[ERROR] JSONDecodeError on doc merged_doc12, retry 1/3: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "Processing document 14: merged_doc13\n",
      "\n",
      "Processing document 15: merged_doc14\n",
      "\n",
      "Processing document 16: merged_doc15\n",
      "\n",
      "Processing document 17: merged_doc16\n",
      "\n",
      "Processing document 18: merged_doc17\n",
      "\n",
      "Processing document 19: merged_doc18\n",
      "\n",
      "Processing document 20: merged_doc19\n",
      "\n",
      "Processing document 21: merged_doc20\n",
      "\n",
      "Processing document 22: merged_doc21\n",
      "\n",
      "Processing document 23: merged_doc22\n",
      "\n",
      "Processing document 24: merged_doc23\n",
      "\n",
      "Processing document 25: merged_doc24\n",
      "\n",
      "Processing document 26: merged_doc25\n",
      "[ERROR] JSONDecodeError on doc merged_doc25, retry 1/3: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "Processing document 27: merged_doc26\n",
      "\n",
      "Processing document 28: merged_doc27\n",
      "[ERROR] JSONDecodeError on doc merged_doc27, retry 1/3: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "Processing document 29: merged_doc28\n",
      "\n",
      "Processing document 30: merged_doc29\n",
      "\n",
      "Processing document 31: merged_doc30\n",
      "\n",
      "Processing document 32: merged_doc31\n",
      "\n",
      "Processing document 33: merged_doc32\n",
      "\n",
      "Processing document 34: merged_doc33\n",
      "[ERROR] JSONDecodeError on doc merged_doc33, retry 1/3: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "Processing document 35: merged_doc34\n",
      "\n",
      "Processing document 36: merged_doc35\n",
      "\n",
      "Processing document 37: merged_doc36\n",
      "\n",
      "Processing document 38: merged_doc37\n",
      "\n",
      "Processing document 39: merged_doc38\n",
      "\n",
      "Processing document 40: merged_doc39\n",
      "[ERROR] JSONDecodeError on doc merged_doc39, retry 1/3: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "Processing document 41: merged_doc40\n",
      "\n",
      "Processing document 42: merged_doc41\n",
      "[ERROR] JSONDecodeError on doc merged_doc41, retry 1/3: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "Processing document 43: merged_doc42\n",
      "\n",
      "Processing document 44: merged_doc43\n",
      "\n",
      "Processing document 45: merged_doc44\n",
      "\n",
      "Processing document 46: merged_doc45\n",
      "\n",
      "Processing document 47: merged_doc46\n",
      "[ERROR] JSONDecodeError on doc merged_doc46, retry 1/3: Expecting value: line 1 column 1 (char 0)\n",
      "[ERROR] JSONDecodeError on doc merged_doc46, retry 2/3: Expecting value: line 1 column 1 (char 0)\n",
      "[ERROR] JSONDecodeError on doc merged_doc46, retry 3/3: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "Processing document 48: merged_doc47\n",
      "\n",
      "Processing document 49: merged_doc48\n",
      "\n",
      "Processing document 50: merged_doc49\n",
      "\n",
      "Processing document 51: merged_doc50\n",
      "\n",
      "Processing document 52: merged_doc51\n",
      "[ERROR] JSONDecodeError on doc merged_doc51, retry 1/3: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "Processing document 53: merged_doc52\n",
      "\n",
      "Processing document 54: merged_doc53\n",
      "\n",
      "Processing document 55: merged_doc54\n",
      "\n",
      "Processing document 56: merged_doc55\n",
      "\n",
      "Processing document 57: merged_doc56\n",
      "\n",
      "Processing document 58: merged_doc57\n",
      "[ERROR] JSONDecodeError on doc merged_doc57, retry 1/3: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "Processing document 59: merged_doc58\n",
      "[ERROR] JSONDecodeError on doc merged_doc58, retry 1/3: Expecting value: line 1 column 1 (char 0)\n",
      "[ERROR] JSONDecodeError on doc merged_doc58, retry 2/3: Expecting value: line 1 column 1 (char 0)\n",
      "[ERROR] JSONDecodeError on doc merged_doc58, retry 3/3: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "Processing document 60: merged_doc59\n",
      "[ERROR] JSONDecodeError on doc merged_doc59, retry 1/3: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "Processing document 61: merged_doc60\n",
      "[ERROR] JSONDecodeError on doc merged_doc60, retry 1/3: Expecting value: line 1 column 1 (char 0)\n",
      "[ERROR] JSONDecodeError on doc merged_doc60, retry 2/3: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "Processing document 62: merged_doc61\n",
      "\n",
      "Processing document 63: merged_doc62\n",
      "[ERROR] JSONDecodeError on doc merged_doc62, retry 1/3: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "Processing document 64: merged_doc63\n",
      "\n",
      "Processing document 65: merged_doc64\n",
      "\n",
      "Processing document 66: merged_doc65\n",
      "[ERROR] JSONDecodeError on doc merged_doc65, retry 1/3: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "Processing document 67: merged_doc66\n",
      "[ERROR] JSONDecodeError on doc merged_doc66, retry 1/3: Expecting value: line 1 column 1 (char 0)\n",
      "[ERROR] JSONDecodeError on doc merged_doc66, retry 2/3: Expecting value: line 1 column 1 (char 0)\n",
      "[ERROR] JSONDecodeError on doc merged_doc66, retry 3/3: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "Processing document 68: merged_doc67\n",
      "\n",
      "Processing document 69: merged_doc68\n",
      "\n",
      "Processing document 70: merged_doc69\n",
      "\n",
      "Processing document 71: merged_doc70\n",
      "[ERROR] JSONDecodeError on doc merged_doc70, retry 1/3: Expecting value: line 1 column 1 (char 0)\n",
      "[ERROR] JSONDecodeError on doc merged_doc70, retry 2/3: Expecting value: line 1 column 1 (char 0)\n",
      "[ERROR] JSONDecodeError on doc merged_doc70, retry 3/3: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "Processing document 72: merged_doc71\n",
      "\n",
      "Processing document 73: merged_doc72\n",
      "\n",
      "Processing document 74: merged_doc73\n",
      "\n",
      "Processing document 75: merged_doc74\n",
      "\n",
      "Processing document 76: merged_doc75\n",
      "\n",
      "Processing document 77: merged_doc76\n",
      "\n",
      "Processing document 78: merged_doc77\n",
      "\n",
      "Processing document 79: merged_doc78\n",
      "\n",
      "Processing document 80: merged_doc79\n",
      "[ERROR] JSONDecodeError on doc merged_doc79, retry 1/3: Expecting value: line 1 column 1 (char 0)\n",
      "[ERROR] JSONDecodeError on doc merged_doc79, retry 2/3: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "Processing document 81: merged_doc80\n",
      "\n",
      "Processing document 82: merged_doc81\n",
      "\n",
      "Processing document 83: merged_doc82\n",
      "\n",
      "Processing document 84: merged_doc83\n",
      "[ERROR] JSONDecodeError on doc merged_doc83, retry 1/3: Expecting value: line 1 column 1 (char 0)\n",
      "[ERROR] JSONDecodeError on doc merged_doc83, retry 2/3: Expecting ',' delimiter: line 165 column 5 (char 3515)\n",
      "[ERROR] JSONDecodeError on doc merged_doc83, retry 3/3: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "Processing document 85: merged_doc84\n",
      "[ERROR] JSONDecodeError on doc merged_doc84, retry 1/3: Expecting value: line 1 column 1 (char 0)\n",
      "[ERROR] JSONDecodeError on doc merged_doc84, retry 2/3: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "Processing document 86: merged_doc85\n",
      "\n",
      "Processing document 87: merged_doc86\n",
      "\n",
      "Processing document 88: merged_doc87\n",
      "[ERROR] JSONDecodeError on doc merged_doc87, retry 1/3: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "Processing document 89: merged_doc88\n",
      "[ERROR] JSONDecodeError on doc merged_doc88, retry 1/3: Expecting value: line 1 column 1 (char 0)\n",
      "[ERROR] JSONDecodeError on doc merged_doc88, retry 2/3: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "Processing document 90: merged_doc89\n",
      "\n",
      "Processing document 91: merged_doc90\n",
      "\n",
      "Processing document 92: merged_doc91\n",
      "\n",
      "Processing document 93: merged_doc92\n",
      "\n",
      "Processing document 94: merged_doc93\n",
      "\n",
      "Processing document 95: merged_doc94\n",
      "\n",
      "Processing document 96: merged_doc95\n",
      "\n",
      "Processing document 97: merged_doc96\n",
      "\n",
      "Processing document 98: merged_doc97\n",
      "[ERROR] JSONDecodeError on doc merged_doc97, retry 1/3: Expecting value: line 1 column 1 (char 0)\n",
      "\n",
      "Processing document 99: merged_doc98\n",
      "\n",
      "Processing document 100: merged_doc99\n",
      "\n",
      "Processing document 101: merged_doc100\n",
      "\n",
      "Processing document 102: merged_doc101\n",
      "\n",
      "Completed. Graph saved at /home/sbhavsar/PoisonedRAG/after_seminar_small_kg/jsons/23_04_2025_knowledge_graph_new_sys.json.\n"
     ]
    }
   ],
   "source": [
    "# File paths\n",
    "input_file_path = \"/home/sbhavsar/PoisonedRAG/datasets/nq/combined_by_title_from after_seminar.jsonl\"\n",
    "output_file_path = \"/home/sbhavsar/PoisonedRAG/after_seminar_small_kg/jsons/23_04_2025_knowledge_graph_new_sys.json\"\n",
    "failed_chunks_file_path = \"/home/sbhavsar/PoisonedRAG/after_seminar_small_kg/jsons/23_04_2025_failed_chunks_new_sys.json\"\n",
    "\n",
    "# Run the processor\n",
    "process_jsonl_file(input_file_path, output_file_path, failed_chunks_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "poisonedRAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
