{
    "model_info": {
        "provider": "llama",
        "name": "meta-llama/Llama-2-7b-chat-hf"
    },
    "api_key_info": {
        "api_keys": [
            ""
        ],
        "api_key_use": 0
    },
    "params": {
        "temperature": 0.1,
        "seed": 100,
        "gpus": [
            0
        ],
        "device": "cuda",
        "max_output_tokens": 150
    }
}